{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5110637",
   "metadata": {},
   "source": [
    "# HW4: NLP Assignment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dac7a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4846 samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "70e0f6aa-f62c-4b2a-9d1b-934c8215f77b",
       "rows": [
        [
         "0",
         "According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .",
         "neutral"
        ],
        [
         "1",
         "Technopolis plans to develop in stages an area of no less than 100,000 square meters in order to host companies working in computer technologies and telecommunications , the statement said .",
         "neutral"
        ],
        [
         "2",
         "The international electronic industry company Elcoteq has laid off tens of employees from its Tallinn facility ; contrary to earlier layoffs the company contracted the ranks of its office workers , the daily Postimees reported .",
         "negative"
        ],
        [
         "3",
         "With the new production plant the company would increase its capacity to meet the expected increase in demand and would improve the use of raw materials and therefore increase the production profitability .",
         "positive"
        ],
        [
         "4",
         "According to the company 's updated strategy for the years 2009-2012 , Basware targets a long-term net sales growth in the range of 20 % -40 % with an operating profit margin of 10 % -20 % of net sales .",
         "positive"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0  According to Gran , the company has no plans t...   neutral\n",
       "1  Technopolis plans to develop in stages an area...   neutral\n",
       "2  The international electronic industry company ...  negative\n",
       "3  With the new production plant the company woul...  positive\n",
       "4  According to the company 's updated strategy f...  positive"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# # Ensure stopwords are downloaded\n",
    "# try:\n",
    "#     nltk.data.find('corpora/stopwords')\n",
    "# except LookupError:\n",
    "#     nltk.download('stopwords')\n",
    "\n",
    "# Load data\n",
    "data = []\n",
    "# Reading with latin-1 to handle potential encoding issues, as is common with some text datasets\n",
    "with open('Sentences_50Agree.txt', 'r', encoding='latin-1') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line: continue\n",
    "        # Split by the last @ to separate text and label\n",
    "        parts = line.rsplit('@', 1)\n",
    "        if len(parts) == 2:\n",
    "            data.append(parts)\n",
    "\n",
    "df = pd.DataFrame(data, columns=['text', 'label'])\n",
    "print(f\"Loaded {len(df)} samples.\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb77035c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Mac GPU)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check for MPS (Metal Performance Shaders) availability for Mac\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Mac GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA (NVIDIA GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4144b5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    # 1. Set seed for Python's built-in random\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # 2. Set seed for NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # 3. Set seed for PyTorch (CPU)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # 4. Set seed for PyTorch (GPU/MPS)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # 5. Force PyTorch to use deterministic algorithms (slower, but reproducible)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af9c3f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples after filtering: 4846\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "processed_text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "3dba2f1b-b9f5-4160-8b43-090e6de12e4a",
       "rows": [
        [
         "0",
         "According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .",
         "neutral",
         "according gran , company plans move production russia , although company growing ."
        ],
        [
         "1",
         "Technopolis plans to develop in stages an area of no less than 100,000 square meters in order to host companies working in computer technologies and telecommunications , the statement said .",
         "neutral",
         "technopolis plans develop stages area less , square meters order host companies working computer technologies telecommunications , statement said ."
        ],
        [
         "2",
         "The international electronic industry company Elcoteq has laid off tens of employees from its Tallinn facility ; contrary to earlier layoffs the company contracted the ranks of its office workers , the daily Postimees reported .",
         "negative",
         "international electronic industry company elcoteq laid tens employees tallinn facility ; contrary earlier layoffs company contracted ranks office workers , daily postimees reported ."
        ],
        [
         "3",
         "With the new production plant the company would increase its capacity to meet the expected increase in demand and would improve the use of raw materials and therefore increase the production profitability .",
         "positive",
         "new production plant company would increase capacity meet expected increase demand would improve use raw materials therefore increase production profitability ."
        ],
        [
         "4",
         "According to the company 's updated strategy for the years 2009-2012 , Basware targets a long-term net sales growth in the range of 20 % -40 % with an operating profit margin of 10 % -20 % of net sales .",
         "positive",
         "according company 's updated strategy years - , basware targets long-term net sales growth range % - % operating profit margin % - % net sales ."
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>according gran , company plans move production...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>technopolis plans develop stages area less , s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>international electronic industry company elco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "      <td>positive</td>\n",
       "      <td>new production plant company would increase ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "      <td>positive</td>\n",
       "      <td>according company 's updated strategy years - ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label  \\\n",
       "0  According to Gran , the company has no plans t...   neutral   \n",
       "1  Technopolis plans to develop in stages an area...   neutral   \n",
       "2  The international electronic industry company ...  negative   \n",
       "3  With the new production plant the company woul...  positive   \n",
       "4  According to the company 's updated strategy f...  positive   \n",
       "\n",
       "                                      processed_text  \n",
       "0  according gran , company plans move production...  \n",
       "1  technopolis plans develop stages area less , s...  \n",
       "2  international electronic industry company elco...  \n",
       "3  new production plant company would increase ca...  \n",
       "4  according company 's updated strategy years - ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove stop words and filter empty words\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply preprocessing\n",
    "df['processed_text'] = df['text'].apply(preprocess)\n",
    "\n",
    "# Only consider documents with at least 1 word\n",
    "df = df[df['processed_text'].str.len() > 0].copy()\n",
    "print(f\"Samples after filtering: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e9dea25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 4361\n",
      "Test set size: 485\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['processed_text'], \n",
    "    df['label'], \n",
    "    test_size=0.1, \n",
    "    stratify=df['label'], \n",
    "    random_state=2013\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b420040a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority class in training data: neutral\n",
      "Base rate accuracy on test data: 0.5938\n"
     ]
    }
   ],
   "source": [
    "# Report the training dataâ€™s base rate multinomial classification accuracy on the test data\n",
    "# Base rate is the accuracy if we simply predict the majority class from the training set\n",
    "\n",
    "# Find majority class in training data\n",
    "majority_class = y_train.mode()[0]\n",
    "print(f\"Majority class in training data: {majority_class}\")\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "# If we predict 'majority_class' for every instance in test set\n",
    "base_rate_accuracy = (y_test == majority_class).mean()\n",
    "print(f\"Base rate accuracy on test data: {base_rate_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6a61de",
   "metadata": {},
   "source": [
    "## 2. Naive Bayes\n",
    "(a) Compute TF-IDF values for every word in your pre-processed text for both the training and test data. Avoid data leakage.\n",
    "(b) Fit the Multinomial Naive Bayes estimator on the training data with Laplace smoothing, then report the accuracy on the test dataset.\n",
    "(c) Describe how the Multinomial Naive Bayes classifier performs against base rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52ca29a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of TF-IDF training matrix: (4361, 8461)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# (a) Compute TF-IDF values\n",
    "# Initialize vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Fit on training data ONLY to avoid data leakage, then transform training data\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "\n",
    "# Transform test data using the vocabulary and IDF learned from training data\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(f\"Shape of TF-IDF training matrix: {X_train_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ee8f3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes Accuracy: 0.6825\n",
      "Base Rate Accuracy: 0.5938\n"
     ]
    }
   ],
   "source": [
    "# (b) Fit Multinomial Naive Bayes with Laplace smoothing\n",
    "# alpha=1.0 is the default in sklearn, which implements Laplace smoothing\n",
    "mnb = MultinomialNB(alpha=1.0)\n",
    "mnb.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_mnb = mnb.predict(X_test_tfidf)\n",
    "\n",
    "# Report accuracy\n",
    "mnb_accuracy = accuracy_score(y_test, y_pred_mnb)\n",
    "print(f\"Multinomial Naive Bayes Accuracy: {mnb_accuracy:.4f}\")\n",
    "print(f\"Base Rate Accuracy: {base_rate_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e850828b",
   "metadata": {},
   "source": [
    "### (c) Comparison\n",
    "The Multinomial Naive Bayes classifier achieved an accuracy of 0.6825, which is higher than the base rate accuracy of 0.5938.\n",
    "\n",
    "This implies that the predictor variables (the words in the text) have a relationship with the response variable (sentiment). The improvement over the base rate indicates that the words in the sentences provide meaningful information that helps distinguish between the sentiment classes, rather than just guessing the most common class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753b2e49",
   "metadata": {},
   "source": [
    "## 3. Feedforward Neural Network\n",
    "(a) Fit a feedforward neural network (MLP) using TF-IDF values. Justify your hyperparameters.\n",
    "(b) Report the test accuracy.\n",
    "(c) Compare performance to Naive Bayes and base rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b71cd161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP (Feedforward NN) Accuracy: 0.7175\n",
      "Multinomial Naive Bayes Accuracy: 0.6825\n",
      "Base Rate Accuracy: 0.5938\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode labels to integers to avoid issues with MLPClassifier early stopping validation\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_test_enc = le.transform(y_test)\n",
    "\n",
    "# (a) Fit MLPClassifier\n",
    "# Justification for hyperparameters:\n",
    "# - hidden_layer_sizes=(100,): The input dimension is high (~8000 words). A layer of 100 neurons allows the model \n",
    "#   to learn a compressed representation of features without being too complex (which would cause overfitting).\n",
    "# - activation='relu': The standard activation function for modern neural networks, helps with training speed and convergence.\n",
    "# - solver='adam': Efficient for large datasets and high-dimensional data.\n",
    "# - early_stopping=True: Crucial for this dataset because we have more features than samples. \n",
    "#   This stops training when validation score stops improving, preventing the model from memorizing the training data.\n",
    "# - max_iter=300: Giving the solver enough time to converge.\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(100,), \n",
    "    activation='relu', \n",
    "    solver='adam', \n",
    "    max_iter=300, \n",
    "    random_state=2013,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "mlp.fit(X_train_tfidf, y_train_enc)\n",
    "\n",
    "# (b) Report test accuracy\n",
    "y_pred_mlp = mlp.predict(X_test_tfidf)\n",
    "mlp_accuracy = accuracy_score(y_test_enc, y_pred_mlp)\n",
    "\n",
    "print(f\"MLP (Feedforward NN) Accuracy: {mlp_accuracy:.4f}\")\n",
    "print(f\"Multinomial Naive Bayes Accuracy: {mnb_accuracy:.4f}\")\n",
    "print(f\"Base Rate Accuracy: {base_rate_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919efbbb",
   "metadata": {},
   "source": [
    "**Hyperparameter Justification:**\n",
    "*   **`hidden_layer_sizes=(100,)`**: The input dimension is high (~8000 words). A single hidden layer of 100 neurons allows the model to learn a compressed representation of features without being too complex, which helps avoid overfitting on this relatively small dataset.\n",
    "*   **`activation='relu'`**: ReLU is the standard activation function for modern neural networks. It helps with training speed and convergence compared to older functions like sigmoid or tanh.\n",
    "*   **`solver='adam'`**: Adam is an efficient optimizer for large datasets and high-dimensional data, handling sparse gradients (like TF-IDF) well.\n",
    "*   **`early_stopping=True`**: This is crucial because we have more features than samples. It stops training when the validation score stops improving, preventing the model from memorizing the training data (overfitting).\n",
    "*   **`max_iter=300`**: Provides the solver enough iterations to converge to a good solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a08ca95",
   "metadata": {},
   "source": [
    "### (c) Comparison\n",
    "The Feedforward Neural Network (MLP) achieved an accuracy of 0.7175, which is higher than both the Multinomial Naive Bayes (0.6825) and the base rate (0.5938).\n",
    "\n",
    "This improvement suggests that the relationship between the predictor variables (words) and the response (sentiment) is non-linear and complex. While Naive Bayes assumes independence between features, the Neural Network can capture interactions between words (e.g., \"not\" + \"good\"), allowing it to model the sentiment more accurately. The fact that it outperforms the simpler models indicates that these complex interactions are present and important in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6679751",
   "metadata": {},
   "source": [
    "## 4. CNN for Text Classification\n",
    "(a) Setup data for PyTorch:\n",
    "*   Tokenize text.\n",
    "*   Define vocabulary (including `<PAD>` and `<UNK>`).\n",
    "*   Convert labels to integers.\n",
    "*   Create custom Dataset class and DataLoader with padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbf580d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 8963\n",
      "Labels classes: ['negative' 'neutral' 'positive']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n",
    "\n",
    "# 1. Tokenize text\n",
    "# Since we already preprocessed (lowercase, removed numbers/stopwords), we can just split by space\n",
    "X_train_tokens = [text.split() for text in X_train]\n",
    "X_test_tokens = [text.split() for text in X_test]\n",
    "\n",
    "# 2. Define Vocabulary\n",
    "# Count words in training data\n",
    "word_counts = Counter()\n",
    "for tokens in X_train_tokens:\n",
    "    word_counts.update(tokens)\n",
    "\n",
    "# Create vocab mapping\n",
    "# Reserve 0 for <PAD> and 1 for <UNK>\n",
    "vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "for word, count in word_counts.items():\n",
    "    if word not in vocab:\n",
    "        vocab[word] = len(vocab)\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "# 3. Convert labels to integers\n",
    "# We already did this with LabelEncoder in the previous step (y_train_enc, y_test_enc)\n",
    "# But let's ensure they are torch tensors later\n",
    "print(f\"Labels classes: {le.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df5e1bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text batch shape: torch.Size([64, 34])\n",
      "Label batch shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# 4. Custom Dataset Class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokens_list, labels, vocab):\n",
    "        self.tokens_list = tokens_list\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.unk_idx = vocab['<UNK>']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokens_list[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Convert tokens to indices, use UNK if not found\n",
    "        indices = [self.vocab.get(token, self.unk_idx) for token in tokens]\n",
    "        \n",
    "        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# 5. Collate Function for Padding\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_text, _label) in batch:\n",
    "        label_list.append(_label)\n",
    "        text_list.append(_text)\n",
    "    \n",
    "    # Pad sequences\n",
    "    # padding_value=0 because <PAD> is 0\n",
    "    text_list = pad_sequence(text_list, batch_first=True, padding_value=0)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.long)\n",
    "    \n",
    "    return text_list, label_list\n",
    "\n",
    "# Initialize Datasets\n",
    "train_dataset = TextDataset(X_train_tokens, y_train_enc, vocab)\n",
    "test_dataset = TextDataset(X_test_tokens, y_test_enc, vocab)\n",
    "\n",
    "# Initialize DataLoaders\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "# Verify a batch\n",
    "for text_batch, label_batch in train_loader:\n",
    "    print(f\"Text batch shape: {text_batch.shape}\")\n",
    "    print(f\"Label batch shape: {label_batch.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e289d931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 987,503 trainable parameters\n",
      "TextCNN(\n",
      "  (embedding): Embedding(8963, 100, padding_idx=0)\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv2d(1, 100, kernel_size=(2, 100), stride=(1, 1))\n",
      "    (1): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
      "    (2): Conv2d(1, 100, kernel_size=(4, 100), stride=(1, 1))\n",
      "  )\n",
      "  (fc): Linear(in_features=300, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# (b) Initialize CNN Network\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes, num_filters, kernel_sizes):\n",
    "        super(TextCNN, self).__init__()\n",
    "        # Embedding Layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # Convolution Layers\n",
    "        # We create a ModuleList of Conv2d layers.\n",
    "        # Input channels = 1 (text is like a grayscale image with 1 channel)\n",
    "        # Output channels = num_filters\n",
    "        # Kernel size = (n_gram_size, embed_dim) -> covers n words across full embedding width\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1, \n",
    "                      out_channels=num_filters, \n",
    "                      kernel_size=(k, embed_dim)) \n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        # Input size = num_filters * number of different kernel sizes\n",
    "        self.fc = nn.Linear(num_filters * len(kernel_sizes), num_classes)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len]\n",
    "        \n",
    "        # 1. Embedding\n",
    "        x = self.embedding(x)  # [batch_size, seq_len, embed_dim]\n",
    "        \n",
    "        # 2. Permute for Conv2d\n",
    "        # Conv2d expects [batch_size, channels, height, width]\n",
    "        # We treat seq_len as height, embed_dim as width\n",
    "        x = x.unsqueeze(1)  # [batch_size, 1, seq_len, embed_dim]\n",
    "        \n",
    "        # 3. Convolutions + ReLU + Max Pooling\n",
    "        # Apply each conv layer, squeeze dimensions, apply ReLU, then max pool\n",
    "        conv_results = []\n",
    "        for conv in self.convs:\n",
    "            # Conv: [batch_size, num_filters, seq_len-k+1, 1]\n",
    "            out = conv(x)\n",
    "            # Squeeze: [batch_size, num_filters, seq_len-k+1]\n",
    "            out = out.squeeze(3)\n",
    "            # ReLU\n",
    "            out = F.relu(out)\n",
    "            # Max Pool over the entire sequence length\n",
    "            # Max Pool: [batch_size, num_filters]\n",
    "            out = F.max_pool1d(out, out.size(2)).squeeze(2)\n",
    "            conv_results.append(out)\n",
    "            \n",
    "        # 4. Concatenate\n",
    "        # [batch_size, num_filters * len(kernel_sizes)]\n",
    "        x = torch.cat(conv_results, 1)\n",
    "        \n",
    "        # 5. Dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 6. Fully Connected\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "# Hyperparameters\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EMBED_DIM = 100       # Standard choice for word embeddings (dense enough to capture meaning)\n",
    "NUM_FILTERS = 100     # Number of features to extract per n-gram size\n",
    "KERNEL_SIZES = [2, 3, 4] # Look for 2-word, 3-word, and 4-word phrases (bi-grams, tri-grams, 4-grams)\n",
    "NUM_CLASSES = 3       # Negative, Neutral, Positive\n",
    "\n",
    "# Initialize Model\n",
    "model = TextCNN(VOCAB_SIZE, EMBED_DIM, NUM_CLASSES, NUM_FILTERS, KERNEL_SIZES)\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fca26b0",
   "metadata": {},
   "source": [
    "**Hyperparameter Choices:**\n",
    "*   **Embedding Dimension (100):** A standard size that balances capturing semantic meaning with computational efficiency.\n",
    "*   **Number of Filters (100):** We want to learn 100 different features for each n-gram size.\n",
    "*   **Kernel Sizes ([2, 3, 4]):** We want the model to look at pairs of words (bi-grams), triplets (tri-grams), and 4-word phrases to capture local context.\n",
    "\n",
    "**(c) Train CNN**\n",
    "Train for 10 epochs and report test performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7791058c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 0.956 | Train Acc: 58.28%\n",
      "\t Val. Loss: 0.820 |  Val. Acc: 64.31%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.763 | Train Acc: 67.50%\n",
      "\t Val. Loss: 0.715 |  Val. Acc: 69.81%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.763 | Train Acc: 67.50%\n",
      "\t Val. Loss: 0.715 |  Val. Acc: 69.81%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.650 | Train Acc: 72.89%\n",
      "\t Val. Loss: 0.680 |  Val. Acc: 69.67%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.650 | Train Acc: 72.89%\n",
      "\t Val. Loss: 0.680 |  Val. Acc: 69.67%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.558 | Train Acc: 77.60%\n",
      "\t Val. Loss: 0.659 |  Val. Acc: 71.80%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.558 | Train Acc: 77.60%\n",
      "\t Val. Loss: 0.659 |  Val. Acc: 71.80%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.473 | Train Acc: 80.77%\n",
      "\t Val. Loss: 0.661 |  Val. Acc: 71.61%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.473 | Train Acc: 80.77%\n",
      "\t Val. Loss: 0.661 |  Val. Acc: 71.61%\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.397 | Train Acc: 84.40%\n",
      "\t Val. Loss: 0.625 |  Val. Acc: 73.95%\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.397 | Train Acc: 84.40%\n",
      "\t Val. Loss: 0.625 |  Val. Acc: 73.95%\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.348 | Train Acc: 86.98%\n",
      "\t Val. Loss: 0.627 |  Val. Acc: 74.09%\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.348 | Train Acc: 86.98%\n",
      "\t Val. Loss: 0.627 |  Val. Acc: 74.09%\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.293 | Train Acc: 88.83%\n",
      "\t Val. Loss: 0.632 |  Val. Acc: 73.61%\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.293 | Train Acc: 88.83%\n",
      "\t Val. Loss: 0.632 |  Val. Acc: 73.61%\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.239 | Train Acc: 91.26%\n",
      "\t Val. Loss: 0.629 |  Val. Acc: 73.56%\n",
      "Early stopping triggered at epoch 9!\n",
      "\n",
      "Final CNN Test Accuracy (Best Model): 0.7395\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.239 | Train Acc: 91.26%\n",
      "\t Val. Loss: 0.629 |  Val. Acc: 73.56%\n",
      "Early stopping triggered at epoch 9!\n",
      "\n",
      "Final CNN Test Accuracy (Best Model): 0.7395\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# (c) Train CNN\n",
    "# device is already defined at the beginning of the notebook\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for text, labels in iterator:\n",
    "        text, labels = text.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        preds = predictions.argmax(dim=1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text, labels in iterator:\n",
    "            text, labels = text.to(device), labels.to(device)\n",
    "            predictions = model(text)\n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            preds = predictions.argmax(dim=1)\n",
    "            acc = (preds == labels).float().mean()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "N_EPOCHS = 10\n",
    "best_valid_loss = float('inf')\n",
    "patience = 3\n",
    "no_improve_count = 0\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {test_loss:.3f} |  Val. Acc: {test_acc*100:.2f}%')\n",
    "    \n",
    "    # Early Stopping Check\n",
    "    if test_loss < best_valid_loss:\n",
    "        best_valid_loss = test_loss\n",
    "        no_improve_count = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "    else:\n",
    "        no_improve_count += 1\n",
    "        if no_improve_count >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}!\")\n",
    "            break\n",
    "\n",
    "# Load best model for final evaluation\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "print(f\"\\nFinal CNN Test Accuracy (Best Model): {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a63176c",
   "metadata": {},
   "source": [
    "### (d) Comparison\n",
    "The CNN achieved a test accuracy of approximately 74.15%, which is the highest among all models tested (Base Rate: 59.38%, Naive Bayes: 68.25%, MLP: 71.75%).\n",
    "\n",
    "This implies that the structure of the input text (the specific order of words) contains valuable information. Unlike Naive Bayes (which ignores order completely) or the MLP on TF-IDF (which only captures order loosely via n-grams if included in TF-IDF, but mostly relies on word counts), the CNN explicitly looks for local patterns (2-word, 3-word, 4-word phrases) regardless of where they appear in the sentence. The fact that the CNN performs best suggests that local context and specific phrases (e.g., \"not good\", \"very happy\") are strong predictors of sentiment in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab041c67",
   "metadata": {},
   "source": [
    "## 5. Sequence Modeling (LSTM)\n",
    "(a) Initialize a sequential model (LSTM) with embedding, LSTM layer, and fully connected layers. Justify choices.\n",
    "(b) Train the model and report test performance.\n",
    "(c) Compare performance to CNN and previous approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d0074c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LSTM model has 1,300,103 trainable parameters\n",
      "TextLSTM(\n",
      "  (embedding): Embedding(8963, 100, padding_idx=0)\n",
      "  (lstm): LSTM(100, 100, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (fc): Linear(in_features=200, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# (a) Initialize LSTM Model\n",
    "class TextLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, n_layers, bidirectional, dropout):\n",
    "        super(TextLSTM, self).__init__()\n",
    "        \n",
    "        # Embedding Layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(embed_dim, \n",
    "                            hidden_dim, \n",
    "                            num_layers=n_layers, \n",
    "                            bidirectional=bidirectional, \n",
    "                            dropout=dropout if n_layers > 1 else 0,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        # If bidirectional, hidden state size is doubled\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, num_classes)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len]\n",
    "        \n",
    "        # 1. Embedding\n",
    "        embedded = self.dropout(self.embedding(x)) # [batch_size, seq_len, embed_dim]\n",
    "        \n",
    "        # 2. LSTM\n",
    "        # output: [batch_size, seq_len, hidden_dim * num_directions]\n",
    "        # hidden: [num_layers * num_directions, batch_size, hidden_dim]\n",
    "        # cell: [num_layers * num_directions, batch_size, hidden_dim]\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # 3. Extract final hidden state\n",
    "        # If bidirectional, we concatenate the final forward and backward hidden states\n",
    "        if self.lstm.bidirectional:\n",
    "            # hidden[-2] is the last forward hidden state\n",
    "            # hidden[-1] is the last backward hidden state\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "            \n",
    "        # 4. Fully Connected\n",
    "        logits = self.fc(hidden)\n",
    "        return logits\n",
    "\n",
    "# Hyperparameters\n",
    "# Using same EMBED_DIM as CNN for fair comparison\n",
    "HIDDEN_DIM = 100      # Number of hidden units in LSTM\n",
    "N_LAYERS = 2          # Number of LSTM layers\n",
    "BIDIRECTIONAL = True  # Use bidirectional LSTM to capture context from both directions\n",
    "DROPOUT = 0.5         # Regularization\n",
    "\n",
    "# Initialize Model\n",
    "lstm_model = TextLSTM(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, NUM_CLASSES, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "\n",
    "print(f\"The LSTM model has {count_parameters(lstm_model):,} trainable parameters\")\n",
    "print(lstm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c42b72",
   "metadata": {},
   "source": [
    "**Hyperparameter Choices:**\n",
    "*   **Type (LSTM):** LSTMs are generally better than vanilla RNNs at capturing long-term dependencies and avoiding vanishing gradient problems.\n",
    "*   **Hidden Units (100):** Chosen to match the embedding dimension and provide sufficient capacity.\n",
    "*   **Bidirectional (True):** Sentiment often depends on the entire sentence context. \"Not good\" requires seeing \"not\" (before) and \"good\" (after) to understand the negation fully. Bidirectional LSTMs read the sentence forwards and backwards.\n",
    "*   **Layers (2):** Stacking layers allows the model to learn more complex hierarchical features.\n",
    "\n",
    "**(b) Train LSTM**\n",
    "Train for 10 epochs and report test performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f272867c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM Model...\n",
      "Epoch: 01\n",
      "\tTrain Loss: 0.925 | Train Acc: 58.33%\n",
      "\t Val. Loss: 0.848 |  Val. Acc: 63.95%\n",
      "Epoch: 01\n",
      "\tTrain Loss: 0.925 | Train Acc: 58.33%\n",
      "\t Val. Loss: 0.848 |  Val. Acc: 63.95%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.834 | Train Acc: 64.11%\n",
      "\t Val. Loss: 0.791 |  Val. Acc: 66.54%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.834 | Train Acc: 64.11%\n",
      "\t Val. Loss: 0.791 |  Val. Acc: 66.54%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.799 | Train Acc: 64.94%\n",
      "\t Val. Loss: 0.763 |  Val. Acc: 68.59%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.799 | Train Acc: 64.94%\n",
      "\t Val. Loss: 0.763 |  Val. Acc: 68.59%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.751 | Train Acc: 67.63%\n",
      "\t Val. Loss: 0.732 |  Val. Acc: 69.17%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.751 | Train Acc: 67.63%\n",
      "\t Val. Loss: 0.732 |  Val. Acc: 69.17%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.700 | Train Acc: 70.72%\n",
      "\t Val. Loss: 0.740 |  Val. Acc: 69.03%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.700 | Train Acc: 70.72%\n",
      "\t Val. Loss: 0.740 |  Val. Acc: 69.03%\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.665 | Train Acc: 71.96%\n",
      "\t Val. Loss: 0.738 |  Val. Acc: 72.39%\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.665 | Train Acc: 71.96%\n",
      "\t Val. Loss: 0.738 |  Val. Acc: 72.39%\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.619 | Train Acc: 73.67%\n",
      "\t Val. Loss: 0.680 |  Val. Acc: 73.17%\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.619 | Train Acc: 73.67%\n",
      "\t Val. Loss: 0.680 |  Val. Acc: 73.17%\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.566 | Train Acc: 75.65%\n",
      "\t Val. Loss: 0.691 |  Val. Acc: 73.17%\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.566 | Train Acc: 75.65%\n",
      "\t Val. Loss: 0.691 |  Val. Acc: 73.17%\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.543 | Train Acc: 78.19%\n",
      "\t Val. Loss: 0.702 |  Val. Acc: 72.44%\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.543 | Train Acc: 78.19%\n",
      "\t Val. Loss: 0.702 |  Val. Acc: 72.44%\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.520 | Train Acc: 79.48%\n",
      "\t Val. Loss: 0.678 |  Val. Acc: 75.90%\n",
      "\n",
      "Final LSTM Test Accuracy (Best Model): 0.7590\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.520 | Train Acc: 79.48%\n",
      "\t Val. Loss: 0.678 |  Val. Acc: 75.90%\n",
      "\n",
      "Final LSTM Test Accuracy (Best Model): 0.7590\n"
     ]
    }
   ],
   "source": [
    "# (b) Train LSTM\n",
    "lstm_model = lstm_model.to(device)\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "# Reset best loss for new training\n",
    "best_valid_loss = float('inf')\n",
    "no_improve_count = 0\n",
    "\n",
    "print(\"Training LSTM Model...\")\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train(lstm_model, train_loader, optimizer, criterion)\n",
    "    test_loss, test_acc = evaluate(lstm_model, test_loader, criterion)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {test_loss:.3f} |  Val. Acc: {test_acc*100:.2f}%')\n",
    "    \n",
    "    # Early Stopping Check\n",
    "    if test_loss < best_valid_loss:\n",
    "        best_valid_loss = test_loss\n",
    "        no_improve_count = 0\n",
    "        torch.save(lstm_model.state_dict(), 'best_lstm_model.pt')\n",
    "    else:\n",
    "        no_improve_count += 1\n",
    "        if no_improve_count >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}!\")\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "lstm_model.load_state_dict(torch.load('best_lstm_model.pt'))\n",
    "test_loss, test_acc = evaluate(lstm_model, test_loader, criterion)\n",
    "print(f\"\\nFinal LSTM Test Accuracy (Best Model): {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5945ba",
   "metadata": {},
   "source": [
    "### (c) Comparison\n",
    "The LSTM model achieved a test accuracy of approximately 73.06%, which is comparable to the CNN (74.14%) and better than the MLP (71.75%) and Naive Bayes (68.25%).\n",
    "\n",
    "**Comparison with CNN:**\n",
    "I am slightly surprised that the LSTM did not outperform the CNN, as LSTMs are theoretically designed to capture long-range dependencies in sequences (e.g., understanding a sentiment that depends on words far apart in the sentence). However, the CNN's superior performance suggests that for this specific dataset (financial news headlines), **local features** (short phrases like \"profit rose\" or \"loss widened\") are more important than long-range global context. The CNN is excellent at detecting these short, informative triggers, while the LSTM might be \"overthinking\" the sequence structure for relatively short sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fc581f",
   "metadata": {},
   "source": [
    "## 6. Fine-tuning Transformers\n",
    "(a) Re-initialize dataset with raw text.\n",
    "(b) Fine-tune DistilBERT (distilbert-base-uncased).\n",
    "(c) Report accuracy.\n",
    "(d) Compare with previous approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b21543b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51b8edf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 4361\n",
      "Test set size: 485\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Split data using the same random state to ensure consistency\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(\n",
    "    df['text'], \n",
    "    df['label'], \n",
    "    test_size=0.1, \n",
    "    stratify=df['label'], \n",
    "    random_state=2013\n",
    ")\n",
    "\n",
    "# Initialize LabelEncoder (re-creating it here in case Section 3 wasn't run)\n",
    "le = LabelEncoder()\n",
    "le.fit(y_train_raw)\n",
    "\n",
    "# Encode labels\n",
    "y_train_bert = le.transform(y_train_raw)\n",
    "y_test_bert = le.transform(y_test_raw)\n",
    "\n",
    "print(f\"Training set size: {len(X_train_raw)}\")\n",
    "print(f\"Test set size: {len(X_test_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64cba974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders created.\n"
     ]
    }
   ],
   "source": [
    "# (b) Fine-tune DistilBERT\n",
    "\n",
    "# 1. Tokenizer\n",
    "# Load the tokenizer for distilbert-base-uncased\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# 2. Custom Dataset for BERT\n",
    "class BertDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Encoding the text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "# Create Datasets\n",
    "train_dataset_bert = BertDataset(X_train_raw, y_train_bert, tokenizer)\n",
    "test_dataset_bert = BertDataset(X_test_raw, y_test_bert, tokenizer)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader_bert = DataLoader(train_dataset_bert, batch_size=16, shuffle=True)\n",
    "test_loader_bert = DataLoader(test_dataset_bert, batch_size=16, shuffle=False)\n",
    "\n",
    "print(\"DataLoaders created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac1b5010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DistilBERT...\n",
      "Starting Epoch 1...\n",
      "  Batch 50/273 processed...\n",
      "  Batch 50/273 processed...\n",
      "  Batch 100/273 processed...\n",
      "  Batch 100/273 processed...\n",
      "  Batch 150/273 processed...\n",
      "  Batch 150/273 processed...\n",
      "  Batch 200/273 processed...\n",
      "  Batch 200/273 processed...\n",
      "  Batch 250/273 processed...\n",
      "  Batch 250/273 processed...\n",
      "Epoch: 01\n",
      "\tTrain Loss: 0.536 | Train Acc: 77.78%\n",
      "\t Val. Loss: 0.363 |  Val. Acc: 85.89%\n",
      "Starting Epoch 2...\n",
      "Epoch: 01\n",
      "\tTrain Loss: 0.536 | Train Acc: 77.78%\n",
      "\t Val. Loss: 0.363 |  Val. Acc: 85.89%\n",
      "Starting Epoch 2...\n",
      "  Batch 50/273 processed...\n",
      "  Batch 50/273 processed...\n",
      "  Batch 100/273 processed...\n",
      "  Batch 100/273 processed...\n",
      "  Batch 150/273 processed...\n",
      "  Batch 150/273 processed...\n",
      "  Batch 200/273 processed...\n",
      "  Batch 200/273 processed...\n",
      "  Batch 250/273 processed...\n",
      "  Batch 250/273 processed...\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.277 | Train Acc: 89.29%\n",
      "\t Val. Loss: 0.402 |  Val. Acc: 85.69%\n",
      "Starting Epoch 3...\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.277 | Train Acc: 89.29%\n",
      "\t Val. Loss: 0.402 |  Val. Acc: 85.69%\n",
      "Starting Epoch 3...\n",
      "  Batch 50/273 processed...\n",
      "  Batch 50/273 processed...\n",
      "  Batch 100/273 processed...\n",
      "  Batch 100/273 processed...\n",
      "  Batch 150/273 processed...\n",
      "  Batch 150/273 processed...\n",
      "  Batch 200/273 processed...\n",
      "  Batch 200/273 processed...\n",
      "  Batch 250/273 processed...\n",
      "  Batch 250/273 processed...\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.163 | Train Acc: 94.44%\n",
      "\t Val. Loss: 0.403 |  Val. Acc: 86.49%\n",
      "Starting Epoch 4...\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.163 | Train Acc: 94.44%\n",
      "\t Val. Loss: 0.403 |  Val. Acc: 86.49%\n",
      "Starting Epoch 4...\n",
      "  Batch 50/273 processed...\n",
      "  Batch 50/273 processed...\n",
      "  Batch 100/273 processed...\n",
      "  Batch 100/273 processed...\n",
      "  Batch 150/273 processed...\n",
      "  Batch 150/273 processed...\n",
      "  Batch 200/273 processed...\n",
      "  Batch 200/273 processed...\n",
      "  Batch 250/273 processed...\n",
      "  Batch 250/273 processed...\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.090 | Train Acc: 97.17%\n",
      "\t Val. Loss: 0.453 |  Val. Acc: 85.89%\n",
      "Starting Epoch 5...\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.090 | Train Acc: 97.17%\n",
      "\t Val. Loss: 0.453 |  Val. Acc: 85.89%\n",
      "Starting Epoch 5...\n",
      "  Batch 50/273 processed...\n",
      "  Batch 50/273 processed...\n",
      "  Batch 100/273 processed...\n",
      "  Batch 100/273 processed...\n",
      "  Batch 150/273 processed...\n",
      "  Batch 150/273 processed...\n",
      "  Batch 200/273 processed...\n",
      "  Batch 200/273 processed...\n",
      "  Batch 250/273 processed...\n",
      "  Batch 250/273 processed...\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.055 | Train Acc: 98.26%\n",
      "\t Val. Loss: 0.592 |  Val. Acc: 83.67%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.055 | Train Acc: 98.26%\n",
      "\t Val. Loss: 0.592 |  Val. Acc: 83.67%\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 3       # Negative, Neutral, Positive\n",
    "\n",
    "# 3. Initialize Model\n",
    "bert_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased', \n",
    "    num_labels=NUM_CLASSES\n",
    ")\n",
    "\n",
    "# Move to device (MPS/CUDA/CPU)\n",
    "bert_model = bert_model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "# We use a smaller learning rate for fine-tuning\n",
    "optimizer = AdamW(bert_model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training Loop\n",
    "def train_bert(model, iterator, optimizer):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        # Move batch to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "        # Print progress every 50 batches\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"  Batch {i + 1}/{len(iterator)} processed...\")\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate_bert(model, iterator):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            acc = (preds == labels).float().mean()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "# Train\n",
    "N_EPOCHS = 5\n",
    "print(\"Training DistilBERT...\")\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f\"Starting Epoch {epoch+1}...\")\n",
    "    train_loss, train_acc = train_bert(bert_model, train_loader_bert, optimizer)\n",
    "    test_loss, test_acc = evaluate_bert(bert_model, test_loader_bert)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {test_loss:.3f} |  Val. Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6c8ca3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final DistilBERT Test Accuracy: 0.8367\n"
     ]
    }
   ],
   "source": [
    "# (c) Report Accuracy\n",
    "print(f\"Final DistilBERT Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb739e3",
   "metadata": {},
   "source": [
    "### (d) Comparison\n",
    "The fine-tuned DistilBERT model achieved the highest accuracy among all models tested, significantly outperforming the Base Rate, Naive Bayes, MLP, CNN, and LSTM.\n",
    "\n",
    "**Performance & Hardware:**\n",
    "Training this model took only about **3 minutes** for 5 epochs. This impressive speed is largely due to using **MPS (Metal Performance Shaders)** acceleration on the **Apple M4 Pro chip**. I love my laptop\n",
    "\n",
    "**Meaning of the Difference:**\n",
    "The superior performance of DistilBERT compared to previous approaches stems from:\n",
    "1.  **Transfer Learning:** Unlike the other models that learned from scratch, DistilBERT started with a massive amount of pre-existing knowledge about English syntax and semantics (pre-trained on Wikipedia).\n",
    "2.  **Contextual Embeddings:** It understands that the meaning of a word changes based on its context (e.g., \"bank\" of a river vs. \"bank\" for money), whereas TF-IDF and standard embeddings are static.\n",
    "3.  **Self-Attention:** The Transformer architecture allows the model to weigh the importance of every word relative to every other word in the sentence simultaneously, capturing complex dependencies more effectively than the sequential processing of LSTMs or the local feature extraction of CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ec946d",
   "metadata": {},
   "source": [
    "## 7. Zero-shot AI Classification\n",
    "(a) Use an external script to query a Large Language Model (Gemini) for zero-shot classification.\n",
    "\n",
    "**AI Tool Used:** Google Gemini (via `run_ai_labeling.py` automation script)\n",
    "\n",
    "**Prompt Used:**\n",
    "```text\n",
    "I have a list of {len(chunk)} financial headlines. Please classify the sentiment of each one as exactly 'negative', 'neutral', or 'positive'. \n",
    "Return the result as a CSV format with a single column named 'predicted_label' containing only the labels in the same order as the input. \n",
    "Do not include the original text in the output. Do not include numbering. Do not include markdown formatting like ```csv.\n",
    "Just the raw list of labels, one per line.\n",
    "\n",
    "Here is the list:\n",
    "[LIST OF SENTENCES]\n",
    "```\n",
    "\n",
    "(b) Load the predictions and report the accuracy.\n",
    "(c) Compare with the best supervised model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da92cc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Directory to store browser session (cookies, login state)\n",
    "BROWSER_PROFILE_DIR = \"./browser_profile\"\n",
    "# Input file (Raw data file)\n",
    "INPUT_FILE = 'Sentences_50Agree.txt'\n",
    "# Output file to be read by the notebook\n",
    "OUTPUT_FILE = 'ai_predictions.csv'\n",
    "# Number of sentences to process in one batch (to avoid hitting character limits)\n",
    "CHUNK_SIZE = 50 \n",
    "# Set to True to run without opening a visible browser window (after you have logged in)\n",
    "HEADLESS = False \n",
    "\n",
    "async def ask_gemini(page, prompt):\n",
    "    \"\"\"\n",
    "    Sends a prompt to Gemini and retrieves the response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Navigate to Gemini\n",
    "        if \"gemini.google.com\" not in page.url:\n",
    "            await page.goto(\"https://gemini.google.com/app\")\n",
    "            await page.wait_for_load_state(\"networkidle\")\n",
    "\n",
    "        # 2. Find the input box\n",
    "        # Gemini uses a contenteditable div\n",
    "        textarea_selector = 'div[contenteditable=\"true\"][role=\"textbox\"], textarea'\n",
    "        textarea = await page.wait_for_selector(textarea_selector, timeout=10000)\n",
    "        \n",
    "        # 3. Fill and send prompt\n",
    "        await textarea.fill(prompt)\n",
    "        await textarea.press(\"Enter\")\n",
    "        \n",
    "        # 4. Wait for the response to generate\n",
    "        print(\"   Waiting for response...\")\n",
    "        \n",
    "        # Wait for the response container\n",
    "        # Gemini structure: <model-response> contains the text\n",
    "        response_selector = 'model-response' \n",
    "        await page.wait_for_selector(response_selector, timeout=30000)\n",
    "        \n",
    "        # Wait for stability\n",
    "        prev_text = \"\"\n",
    "        stable_count = 0\n",
    "        for _ in range(60): # Wait up to 60 seconds for stability\n",
    "            await asyncio.sleep(1)\n",
    "            elements = await page.query_selector_all(response_selector)\n",
    "            if not elements: continue\n",
    "            \n",
    "            last_element = elements[-1]\n",
    "            curr_text = await last_element.inner_text()\n",
    "            \n",
    "            if curr_text == prev_text and len(curr_text) > 10:\n",
    "                stable_count += 1\n",
    "                if stable_count >= 3: # Stable for 3 seconds\n",
    "                    return curr_text\n",
    "            else:\n",
    "                stable_count = 0\n",
    "            \n",
    "            prev_text = curr_text\n",
    "            \n",
    "        return prev_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   Error querying Gemini: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "async def main():\n",
    "    # 1. Check for Input File\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"âŒ Error: '{INPUT_FILE}' not found.\")\n",
    "        return\n",
    "\n",
    "    # 2. Load and Split Data (Replicating Notebook Logic)\n",
    "    print(f\"ðŸ“‚ Loading data from {INPUT_FILE}...\")\n",
    "    \n",
    "    # Parse the raw text file\n",
    "    data = []\n",
    "    try:\n",
    "        with open(INPUT_FILE, 'r', encoding='latin-1') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line: continue\n",
    "                # Split by the last @ to separate text and label\n",
    "                parts = line.rsplit('@', 1)\n",
    "                if len(parts) == 2:\n",
    "                    data.append(parts)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error reading file: {e}\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['text', 'label'])\n",
    "    \n",
    "    # Perform the exact same split as the notebook to get the TEST set\n",
    "    print(\"   Splitting data (test_size=0.1, random_state=2013)...\")\n",
    "    try:\n",
    "        _, X_test, _, y_test = train_test_split(\n",
    "            df['text'], \n",
    "            df['label'], \n",
    "            test_size=0.1, \n",
    "            stratify=df['label'], \n",
    "            random_state=2013\n",
    "        )\n",
    "    except ImportError:\n",
    "        print(\"âŒ Error: scikit-learn is not installed. Please run 'pip install scikit-learn'\")\n",
    "        return\n",
    "\n",
    "    texts = X_test.tolist()\n",
    "    print(f\"   Found {len(texts)} test sentences to classify.\")\n",
    "    \n",
    "    # Save ground truth for the notebook to use later\n",
    "    ground_truth_df = pd.DataFrame({'text': X_test, 'true_label': y_test})\n",
    "    ground_truth_df.to_csv('test_sentences_ground_truth.csv', index=False)\n",
    "    print(\"   Saved ground truth to 'test_sentences_ground_truth.csv'.\")\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    # 3. Launch Browser\n",
    "    print(\"ðŸš€ Launching Browser...\")\n",
    "    async with async_playwright() as p:\n",
    "        # Create/Load persistent context to keep login session\n",
    "        if not os.path.exists(BROWSER_PROFILE_DIR):\n",
    "            os.makedirs(BROWSER_PROFILE_DIR)\n",
    "            \n",
    "        context = await p.chromium.launch_persistent_context(\n",
    "            user_data_dir=BROWSER_PROFILE_DIR,\n",
    "            headless=HEADLESS,\n",
    "            channel=\"chrome\", # Uses your installed Chrome if available\n",
    "            args=[\"--disable-blink-features=AutomationControlled\"]\n",
    "        )\n",
    "        \n",
    "        page = await context.new_page()\n",
    "        \n",
    "        # Check if logged in\n",
    "        await page.goto(\"https://gemini.google.com/app\")\n",
    "        await asyncio.sleep(3)\n",
    "        if \"accounts.google.com\" in page.url or \"ServiceLogin\" in page.url:\n",
    "            print(\"\\nâš ï¸  PLEASE LOG IN TO GEMINI IN THE BROWSER WINDOW.\")\n",
    "            print(\"   The script will wait for you to log in.\")\n",
    "            input(\"   Press Enter here once you are logged in and can see the chat interface...\")\n",
    "\n",
    "        # 4. Process in Chunks\n",
    "        total_chunks = (len(texts) + CHUNK_SIZE - 1) // CHUNK_SIZE\n",
    "        \n",
    "        for i in range(0, len(texts), CHUNK_SIZE):\n",
    "            chunk_idx = i // CHUNK_SIZE + 1\n",
    "            chunk = texts[i:i+CHUNK_SIZE]\n",
    "            print(f\"\\nðŸ”„ Processing Batch {chunk_idx}/{total_chunks} ({len(chunk)} sentences)...\")\n",
    "            \n",
    "            # Construct the Prompt\n",
    "            prompt = f\"\"\"\n",
    "I have a list of {len(chunk)} financial headlines. Please classify the sentiment of each one as exactly 'negative', 'neutral', or 'positive'. \n",
    "Return the result as a CSV format with a single column named 'predicted_label' containing only the labels in the same order as the input. \n",
    "Do not include the original text in the output. Do not include numbering. Do not include markdown formatting like ```csv.\n",
    "Just the raw list of labels, one per line.\n",
    "\n",
    "Here is the list:\n",
    "\"\"\"\n",
    "            for text in chunk:\n",
    "                prompt += f\"{text}\\n\"\n",
    "            \n",
    "            # Query AI\n",
    "            response_text = await ask_gemini(page, prompt)\n",
    "            \n",
    "            # Parse Response using Regex to be robust against formatting (spaces, newlines, commas)\n",
    "            # We look for the specific words: negative, neutral, positive\n",
    "            found_labels = re.findall(r'\\b(negative|neutral|positive)\\b', response_text, re.IGNORECASE)\n",
    "            \n",
    "            # Convert to lowercase\n",
    "            batch_labels = [l.lower() for l in found_labels]\n",
    "            \n",
    "            print(f\"   Received {len(batch_labels)} labels from AI.\")\n",
    "            \n",
    "            # Handle mismatches (AI sometimes misses one or adds extra)\n",
    "            if len(batch_labels) != len(chunk):\n",
    "                print(f\"   âš ï¸  Warning: Count mismatch (Sent {len(chunk)}, Got {len(batch_labels)}). Padding/Truncating.\")\n",
    "                # If too few, pad with 'neutral'\n",
    "                if len(batch_labels) < len(chunk):\n",
    "                    batch_labels.extend(['neutral'] * (len(chunk) - len(batch_labels)))\n",
    "                # If too many, truncate\n",
    "                else:\n",
    "                    batch_labels = batch_labels[:len(chunk)]\n",
    "            \n",
    "            all_predictions.extend(batch_labels)\n",
    "            \n",
    "            # Small pause to be nice to the server\n",
    "            await asyncio.sleep(2)\n",
    "\n",
    "        await context.close()\n",
    "\n",
    "    # 5. Save Results\n",
    "    print(f\"\\nðŸ’¾ Saving results to {OUTPUT_FILE}...\")\n",
    "    output_df = pd.DataFrame({'predicted_label': all_predictions})\n",
    "    output_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(\"âœ… Done! You can now run the 'Evaluate' cell in your notebook.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9a1b1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot AI (Gemini) Accuracy: 0.7938\n",
      "DistilBERT Accuracy: 0.8367\n"
     ]
    }
   ],
   "source": [
    "# Load the ground truth and predictions from the external script\n",
    "try:\n",
    "    ground_truth_df = pd.read_csv('test_sentences_ground_truth.csv')\n",
    "    predictions_df = pd.read_csv('ai_predictions.csv')\n",
    "\n",
    "    # Ensure lengths match\n",
    "    if len(ground_truth_df) != len(predictions_df):\n",
    "        print(f\"Warning: Length mismatch. Ground Truth: {len(ground_truth_df)}, Predictions: {len(predictions_df)}\")\n",
    "        # Truncate to the shorter one for calculation\n",
    "        min_len = min(len(ground_truth_df), len(predictions_df))\n",
    "        ground_truth_df = ground_truth_df.iloc[:min_len]\n",
    "        predictions_df = predictions_df.iloc[:min_len]\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    # We need to ensure labels are normalized (lowercase, stripped)\n",
    "    y_true = ground_truth_df['true_label'].str.lower().str.strip()\n",
    "    y_pred = predictions_df['predicted_label'].str.lower().str.strip()\n",
    "\n",
    "    ai_accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Zero-shot AI (Gemini) Accuracy: {ai_accuracy:.4f}\")\n",
    "    \n",
    "    # Compare with best previous model (DistilBERT)\n",
    "    # Note: 'test_acc' variable holds the last calculated accuracy (DistilBERT)\n",
    "    print(f\"DistilBERT Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Could not find 'test_sentences_ground_truth.csv' or 'ai_predictions.csv'.\")\n",
    "    print(\"Please run the 'run_ai_labeling.py' script first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691ebb42",
   "metadata": {},
   "source": [
    "### (c) Comparison\n",
    "The Zero-shot AI model (Gemini) achieved an accuracy of **79.38%**.\n",
    "\n",
    "**Comparison with Previous Approaches:**\n",
    "*   **Base Rate:** 59.38%\n",
    "*   **Naive Bayes:** 68.25%\n",
    "*   **MLP:** 71.75%\n",
    "*   **CNN:** 74.15%\n",
    "*   **LSTM:** 73.06%\n",
    "*   **DistilBERT (Fine-tuned):** 83.67%\n",
    "*   **Zero-shot Gemini:** 79.38%\n",
    "\n",
    "**Analysis:**\n",
    "The Zero-shot Gemini model performed exceptionally well, outperforming all traditional supervised models (Naive Bayes, MLP, CNN, LSTM) without seeing a single training example from this dataset. It achieved nearly 80% accuracy purely based on its pre-trained knowledge of language and sentiment.\n",
    "\n",
    "However, it did not beat the fine-tuned DistilBERT model (83.67%). This makes sense because DistilBERT was allowed to study the specific training data for this task, learning the exact vocabulary and style of these financial headlines (the \"specialist\" approach). Gemini, while a powerful \"generalist,\" had to guess the sentiment based on general principles without knowing the specific quirks of this dataset.\n",
    "\n",
    "**Conclusion:**\n",
    "Off-the-shelf LLMs are incredibly powerful baselines that can beat complex custom models (like CNNs/LSTMs) with zero effort. However, for maximum performance, fine-tuning a smaller model (like DistilBERT) on domain-specific data is still the superior approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
